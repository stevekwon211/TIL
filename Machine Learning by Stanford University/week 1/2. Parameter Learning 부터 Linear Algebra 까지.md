# 👤Gradient Descent(경사 하강법)이란?

- **`Gradient Descent(경사하강법)`**은 비용함수를 최소화하기 위한 알고리즘입니다.
  단지 선형회귀뿐만이 아닌 머신러닝 전반적으로 사용된다고 합니다.

  > 선형회귀와 비용함수가 무엇인지 모르겠다면? [여기](https://velog.io/@kwonhl0211/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%84%A0%ED%98%95-%ED%9A%8C%EA%B7%80%EB%8A%94-%EB%AD%90%EA%B3%A0-%EB%B9%84%EC%9A%A9%ED%95%A8%EC%88%98%EB%8A%94-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80%EC%9A%94)를 클릭하세요!

<br>

***

먼저 경사하강법의 수식에 대해 알려드리겠습니다.
$$
repeat \ until \ convergence(수렴)\{ \\
  \theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1) \\
  for \ j=0 \ and \ j=1) \\
  \}
$$
  위 식에서 $$:=$$는 할당한다는 의미입니다. (프로그래밍에서 = 를 생각하시면 됩니다.)

  자세한 의미에 대해선 아래에서 설명됩니다.

<br>

***

## 경사하강법 그래프 & 수학적 정의

아래 그래프는 $$\theta_0$$와 $$\theta_1$$에 따른 비용함수 값을 보여주는 3차원 그래프입니다.
그래프의 빨간색 부분은 비용함수 값이 높은 것을 의미하고 반대로 파란색 부분은 비용함수 값이 낮은 것을 뜻합니다.
비용함수를 최소화하는 값( $$\theta$$)을 찾는 것이 **`경사하강법`** 알고리즘의 목적입니다.

<img src="https://user-images.githubusercontent.com/61633137/104442857-02501900-55d9-11eb-8e15-ecab15e19aa9.png" alt="image" style="zoom:71%;" /> 

<br>

***

또한 경사하강법은 위아래 그래프에서 볼 수 있는 것처럼 $$\theta$$값에 따라 알고리즘이 시작되는 위치가 이동될 수 있습니다.
그리고 시작하는 위치에 따라 마지막에 도달하는 위치도 달라지게 될 수 있습니다.

![image](https://user-images.githubusercontent.com/61633137/104438740-acc53d80-55d3-11eb-9ba3-fc67318573b2.png) 

그래프에 대해 좀 더 설명해보자면 빨간색 점(동그라미)은 탄젠트 함수의 기울기(미분항)를 의미하며 이 점이 계속 이동을 하는 것입니다. 강의에선 비용함수가 하강을 거듭할수록 경사에 점점 스며든다고 표현을 했는데요, 여기서 얼마나 하강할지는 $$\alpha$$ 변수에 의해 결정되며 이를 **`Learning Rate(학습률)`**이라고 합니다.

이 학습률($$\alpha$$)이 작을수록 하강도 작아지며 반대로 학습률이 클수록 하강도 커집니다.
하강에서 방향은 $$\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)$$에 의해 결정됩니다.

<br>

***

이제 위 식을 다시 살펴보겠습니다.
$$
repeat \ until \ convergence(수렴)\{ \\
  \theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1) \\
  for \ j=0 \ and \ j=1) \\
  \}
$$
경사하강법 알고리즘이 계속 반복되면서 $$_j$$가 1씩 증가할 것이고 그에 따라 $$\theta_j$$도 $$\theta_1, \theta_2, ... \theta_n$$ 처럼 계속해서 새로운 값이 대입될 것입니다. 

이 식에서 주의해야 할 것은 $$\theta_0$$과 $$\theta_1$$값이 동시에 업데이트되어야 한다는 것인데요. 
아래 사진에서 왼쪽을 보시면 temp0과 temp1에 값을 할당해두고 나중에 $$\theta_0$$과 $$\theta_1$$에 동시에 값을 할당해 주는 것을 볼 수 있습니다.

![image](https://user-images.githubusercontent.com/61633137/104451673-36313b80-55e5-11eb-8de5-e89d55a2711b.png) 

사진의 오른쪽 식이 잘못된 이유는 이 순서대로 진행될 경우 $$\theta_1$$의 편미분을 구하는 수식에서 바뀐 $$\theta_0$$값이 적용되기 때문입니다.

<br>

***

## 경사하강법의 자세한 예시

- 이제 위에서 설명했던 수식을 그래프와 함께 좀 더 자세하게 알아보겠습니다.
  여기선 $$\theta_1$$ 값을 이용한 예시를 들었습니다.

$$
\theta_1 := \theta_1 - \alpha\frac{\partial}{\partial\theta_1}J(\theta_1) \\
$$

아래에 두 개의 그래프 그림이 있는데요, 왼쪽 그림에서 $$\theta_1$$의 비용함수가 파란 점입니다. 

![image](https://user-images.githubusercontent.com/61633137/104805670-2730d000-5815-11eb-8700-1f3e2521a7f0.png) <img src="https://user-images.githubusercontent.com/61633137/104805713-7b3bb480-5815-11eb-8c79-0362d06c0725.png" alt="image" style="zoom:98%;" />

오른쪽 그림에서 파란색 화살표가 가리키는 부분이 비용함수의 최솟값을 나타냅니다.
빨간색 선은 $$\frac{\partial}{\partial\theta_1}J(\theta_1)$$의 값, 즉 기울기를 나타냅니다.
이 기울기가 양수이므로 학습률 $$\alpha$$에 양수를 곱해준 뒤, 기존 $$\theta_1$$값에서 양수 값을 빼주게 되고 $$\theta_1$$ 값은 X축 상에서 왼쪽으로 이동하게 됩니다.

<br>

아래 그림을 보면 반대로 기울기가 음수인 경우엔 $$\theta_1$$ 값에 음수 값을 빼주게 되니 $$\theta_1$$ 값은 증가하여 오른쪽으로 이동하게 됩니다.

![image](https://user-images.githubusercontent.com/61633137/104805895-b68ab300-5816-11eb-8ce5-a52cd87dd6a8.png) 

<br>

***

## 학습률은 무엇이고 경사하강법에 어떤 영향을 주나요?

- 위에서 잠시 언급되었던 **`Learning Rate(학습률)`** $$\alpha$$는 경사하강법 알고리즘이 적절한 시간에 수렴되도록 것인데요. 
  이 최솟값에 수렴하지 못하거나 너무 많은 시간이 소요되면 학습률의 값이 잘못되었음을 의미합니다.

<br>

아래 첫 번째 그래프는 학습률 $$\alpha$$값이 너무 작아서 경사하강이 느려진 경우를 보여주며,
두 번째 그래프는 학습률 $$\alpha$$값이 너무 커서 최솟값에 수렴하지 못하고 값이 튀어 다니는 경우를 보여줍니다.

<img src="https://user-images.githubusercontent.com/61633137/104806171-dde27f80-5818-11eb-8ecc-809d757d84b2.png" alt="image" style="zoom:120%;" /> <img src="https://user-images.githubusercontent.com/61633137/104806348-2189b900-581a-11eb-89a7-771e91e71130.png" alt="image" style="zoom:100%;" /> 

<br>

***

학습률과 경사하강법에 대해 추가 설명을 덧붙여 보겠습니다. 
$$\theta_1$$의 값이 아래 그래프 X축 위치라고 가정하면 **`local optima(지역 최솟값)`**을 가지는 $$\theta$$값이 되며 기울기가 0이 됩니다.
이런 상태에서 경사하강을 시도하면 기울기가 0이므로 $$\theta$$값은 변하지 않습니다.

![image](https://user-images.githubusercontent.com/61633137/104806684-bc839280-581c-11eb-946b-744caa629704.png) 

<br>

또한 학습률 $$\alpha$$를 고정 값으로 사용하더라도 경사하강법은 지역 최솟값으로 수렴할 수 있습니다.
아래 그래프를 보면 각 화살표가 표시된 지점마다 기울기가 점점 완만해집니다. 즉 기울기가 0에 가까워지게 되며 학습률을 고정으로 하더라도 경사하강법 알고리즘은 점차 작은 값을 $$\theta$$값에 업데이트하게 됩니다. 

![image](https://user-images.githubusercontent.com/61633137/104806904-3b2cff80-581e-11eb-80ef-7932c26f1cda.png) 

<br>

***

# 경사하강법과 Linear Regression(선형 회귀) 알고리즘

- 경사하강법을 선형 회귀 모델에 적용하면 비용함수 $$J$$를 최소화하는 $$\theta$$값을 구하는 것이 목표가 됩니다.
  선형 회귀 모델에 적용했을 시 수식은 다음과 같이 쓸 수 있습니다. [선형 회귀 모델이 뭔지 모르겠다면 클릭!](https://velog.io/@kwonhl0211/머신러닝-선형-회귀는-뭐고-비용함수는-무엇인가요)

  $$
  repeat \ until \ convergence\{ \\
    \theta_0 := \theta_0 - \alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x_i)-y_i) \\ 
    \theta_1 := \theta_1 - \alpha\frac{1}{m}\sum^m_{i=1}((h_\theta(x_i)-y_i)x_i) \\
    \}
  $$

위 수식에 대해 약간의 설명을 붙여보자면 첫 번째 줄은 $$\theta_0에 \ 대한 \ 편미분 \ 값을 \ 학습률 \ \alpha와 \ 곱하고 \ \theta_0에서 \ 뺀 \ 것$$이고 두 번째 줄은 $$\theta_1에 \ 대한 \ 편미분 \ 값을 \ 학습률 \ \alpha와 \ 곱하고 \ \theta_1에서 \ 뺀 \ 것$$이다.

경사 하강법은 [경사하강법 그래프 &  수학적 정의](#경사하강법-그래프--수학적-정의)의 그래프에서 볼 수 있듯이 시작점에 따라 다른 지역 최솟값으로 구해집니다.
하지만 다행히도 선형 회귀의 비용함수는 **`Convex Function(볼록함수)`** 모양이므로 시작 위치에 상관없이 항상 글로벌 최솟값으로 수렴하게 됩니다. 아래 그래프는 선형 회귀의 비용함수 그래프입니다.
<img src="https://user-images.githubusercontent.com/61633137/104318478-d2ded500-5522-11eb-9b71-7d9d97174ca1.png" alt="image" style="zoom:67%;" />

그래서 경사 하강법을 선형 회귀 모델에 적용했을 시 글로벌 최솟값으로 수렴하는 그래프는 다음 예시처럼 그려집니다.
![image](https://user-images.githubusercontent.com/61633137/104807822-459ec780-5825-11eb-8f0e-6c11d5bb0c31.png)

<br>

***
> Coursera에 있는 Andrew Ng 교수님의 Machine Learning 강의를 듣고 정리하는 시리즈입니다!

> 저는 수학 베이스가 부족하여 글의 내용이 틀릴 수도 있습니다. 잘못된 내용이 있을 시 댓글로 알려주시면 감사하겠습니다!!
> (수학 공부는 열심히 하고 있어요! 😁)