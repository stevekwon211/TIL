> Coursera에 있는 Andrew Ng 교수님의 Machine Learning 강의를 듣고 정리하는 시리즈입니다!

> 저는 수학 베이스가 부족하여 글의 내용이 틀릴 수도 있습니다. 잘못된 내용이 있을 시 댓글로 알려주시면 감사하겠습니다!!
> (수학 공부는 열심히 하고 있어요! 😁)

# 👤머신 러닝(Machine Learning)이란?

> 컴퓨터에 분명하게 프로그래밍되지 않고 스스로 학습할 수 있는 능력을 부여하는 분야이다
>
> \- Arthur Samuel

> 수행하는 과제 T와 성능 측정방식 P를 준수하는 경험 E로부터 배우는 컴퓨터 프로그램을 말하며
> P에 의해 측정되는 과제 T의 성과가 경험 E에 따라 개선된다. 
>
> \- Tom Mitchell

`Tom Mitchell`의 말에 따라 한 가지 예시로 `스팸 메일📨`을 들어보겠습니다.

- **T**ask : 스팸 메일인지 아닌지 분류
- **E**xperience :  스팸 메일인지 아닌지 분류한 경험
- **P**erformance : 스팸인지 아닌지 올바르게 분류한 메일의 수

정리해보자면 스팸 메일인지 아닌지 분류한 것(**T**ask)을 올바르게 분류한 메일의 수로 측정(**P**erformance)하고 그로부터 얻은 경험 (**E**xperience)을 통해 배우는 프로그램을 머신러닝이라고 할 수 있겠네요.

<br>

일반적으로 머신러닝은 두 가지로 구분지을 수 있습니다.

- 지도 학습(Supervised Learning)
- 비지도 학습(Unsupervised Learning)

<br>

## 지도 학습(Supervised Learning)이란?

- 지도 학습에서는 **`정답이 있는 데이터`**를 갖고 시작합니다. 
  데이터에 정답이 이미 존재하기에 어떤 결과를 얻어야할지 알고 있으며, 데이터들의 입출력으로부터 어떤 관계가 있는지에 대해 생각해보아야 합니다.

![image](https://user-images.githubusercontent.com/61633137/104239795-6b7f4180-549e-11eb-90ea-2aa1c62b0372.png) 
**`회귀`** 예시 : 집의 가격을 예측

<img src="https://user-images.githubusercontent.com/61633137/104240016-bac57200-549e-11eb-99a7-d7cfb9c2e503.png" alt="image" style="zoom: 104%;" /> 
**`분류`** 예시 : 유방암의 종양 크기에 따른 악성 종양 분류

- 지도학습은 다시 한 번 두 가지로 나누어볼 수 있는데요, 바로 **`회귀(Regression)`**와 **`분류(Classification)`**입니다.
  **`회귀`**는 예측해야하는 데이터가 `연속형`인 경우에 사용되며 **`분류`**는 `이산형` 데이터를 예측할 때 사용됩니다.
  - 여기서 `연속형 데이터`란 연속된 무수히 많은 데이터 중 하나를 의미합니다.
  - 그리고  `이산형 데이터`란 수치적인 의미를 가지지 못하며 소수점의 형태로 표현되지 못하는 값을 의미합니다(정수로 딱 떨어지는 데이터 : 1 or 0). 

<br>

## 비지도 학습(Unsupervised Learning)이란?

- 비지도 학습에서는 어떤 결과값을 얻어야할지, 그리고 결과값이 어떻게 나올지에 대해 알기 어려운 문제에서 사용됩니다.
  한 마디로 **`정답이 없는 데이터`**를 다룹니다.
- 이러한 정답이 없는 데이터로부터 특정 구조를 파생하는데 이를 '데이터를 **`클러스터링(Clustering)`**한다' 고 말합니다.

![image](https://user-images.githubusercontent.com/61633137/104240476-771f3800-549f-11eb-9c7d-fc59b6fcecd3.png) 
**`지도 학습`**의 경우 정답이 있는 데이터를 이용하여 두 가지로 분류하는 것을 볼 수 있으며 

![image](https://user-images.githubusercontent.com/61633137/104240553-98802400-549f-11eb-8b00-fb23b91cdf26.png) 
**`비지도 학습`**의 경우 정답이 없는 데이터를 이용하기 때문에 **`군집(Cluster)`**으로 분리하게 됩니다.

- 또한 비지도 학습은 **`군집화(Clustering)`**과 **`Non-clustering`**으로 구분지을 수 있습니다. 
  - **`Clustering`** 예시
    1,000,000개의 서로 다른 유전자들을 모아서, 이 유전자들을 수명, 위치, 역할 등과 같은 다른 변수들에 의해 어떻게든 유사하거나 관련이 있는 그룹으로 자동 분류
  - **`Non-clustering`** 예시
    비군집화(Non-clustering) 문제의 유명한 예시는 **`칵테일 파티🍸`**입니다. 파티에 두 명의 사람이 있고, 두 위치에서 녹음된 두 개의 마이크가 있다고 가정합니다. 마이크 1은 사람 1에 가깝고, 마이크 2는 사람 2에 가깝습니다. 두 사람 모두 마이크를 이용해 대화를 이어가며 이를 통해 녹음된 음성으로부터 두 목소리를 분리시키는 것입니다. 

위 두 예시가 이해되시나요? 저는 처음에 **`Clustering`**과 **`Non-clustering`**의 차이점이 이해가 잘 되지 않아서 좀 더 찾아보았습니다.
제가 이해한 것을 말씀드려보자면 **`Clustering`**은 정리되어 있는 데이터를 군집화하는 것, **`Non-clustering`**은 정리되어 있지 않은 데이터를 군집화하는 것입니다.

혹시 제가 이해한 것이 맞지 않다면 댓글로 알려주시면 감사하겠습니다! 👨‍💻

<br>

# 선형 회귀 모델(Model) 과 비용함수(Cost Function)

## 모델 표현 - Notation & Algorithm

- 지도 학습에서 나왔던 **`회귀`** 문제로 되돌아 왔습니다. **`회귀`** 에서 사용하는 Notation은 다음과 같습니다.

$$
입력 \ 변수 : x^{(i)} \\
출력 \ 변수 : y^{(i)} \\
훈련(traing) 예시 : (x^{(i)},y^{(i)});i = 1, ...,m \\
m은 \ 훈련(traing)의 \ 횟수(개수)를 \ 뜻합니다.
$$

- 지도학습 알고리즘은 다음 그림처럼 작동합니다.

![Untitled Diagram (1)](https://user-images.githubusercontent.com/61633137/103527035-18692580-4ec5-11eb-9ef8-2313d878379b.png)

- 먼저 모델을 훈련할 **`훈련 셋(Training Set)`**을 **`학습 알고리즘(Learning Algorithm)`**에 넣어줍니다.
  그러면 이 **`학습 알고리즘`**이 **`가설 함수(hypothesis function) h`** 를 만들게 되는데요.
  이름이 왜 가설 함수인지는 Andrew Ng 교수님도 신경쓰지 말라고 하더군요 😂
  $$
  h_\theta(x) = \theta_0 + \theta_1x \\
  \hat{y} = h_\theta(x)
  $$
   위 식이 가설 함수입니다.

<br>

## 모델 예시

> *그렇다면 위 알고리즘과 Notation들은 어떻게 사용되나요?*

강의에 나온 예시인 **`집 가격 예측`** 으로 설명해보겠습니다.

-  아래 사진은 집의 크기(Size)에 따라 가격(Price)이 어떻게 변화하는지에 대한 그래프입니다.
  ![image](https://user-images.githubusercontent.com/61633137/104244877-a4bbaf80-54a6-11eb-9019-e5a73fdf2220.png)

<br>

- 아래 사진은 Size와 Price 데이터를 담고 있는 **`훈련 셋(Training Set)`**입니다.
  ![image](https://user-images.githubusercontent.com/61633137/104245248-46db9780-54a7-11eb-8c63-fe7b2c566c56.png)

<br>

- 여기서 Size와 Price는 각각 입력 변수와 출력 변수를 뜻합니다.
  $$
  입력 \ 변수 = Size \ in \ feet^2 : x \\
  출력 \ 변수 = Price (\$) in \ 1000's  : y \\
  $$

- 그리고 가설 함수를 다시 살펴보겠습니다.
  $$
  h_\theta(x) = \theta_0 + \theta_1x \\
  \hat{y} = h_\theta(x)
  $$
  여기서 입력 변수인 Size(x)가 들어가고 그에 따라 출력 변수인 Price(y)를 얻는 것입니다. 
  <img src="https://user-images.githubusercontent.com/61633137/104245905-70e18980-54a8-11eb-90a7-7f1de61ce715.png" alt="image" style="zoom:150%;" />

이러한 모델을 **`Linear Regression with one variable`**, 즉 **`Univariate Linear Regression(단변량 선형 회귀)`**이라고 합니다.
<br>

## 비용함수 (Cost Function)

![image](https://user-images.githubusercontent.com/61633137/104305062-8c33af80-550f-11eb-80aa-5131608ac6f5.png) 

- $$y$$와 $$h(x)$$의 차가 적게 만드는 $$θ$$ 값을 찾아내는 것이 중요한데 그것을 알 수 있는 방법이 **`비용함수(Cost Function)`**입니다.
  비용함수는 가설 함수의 정확도를 측정할 수 있는 함수이며, 가설 함수로 부터 나온 결과 값과 실제 결과 $$y$$의 차이의 평균을 구한 것입니다.
- 비용함수의 식은 다음과 같습니다.

$$
J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2
$$

- m은 훈련의 수를 뜻합니다. 그렇다면 평균을 구하기 위해서는 $$m$$으로 나누어야 하는데 왜 $$2m$$으로 나눌까요?

  ![image](https://user-images.githubusercontent.com/61633137/104303775-dc117700-550d-11eb-872d-60725f43a7b3.png)

  나중에 편미분을 할 시 방정식의 분자에 나타나는 2를 지우기 위해서, 그것이 단순히 수학적으로 편리하기 떄문이랍니다.

- 비용함수는 다른 이름으로도 불리는데요, 바로 **`제곱 오차(Squared Error Function)`** 와 **`평균 제곱 오차(Mean Squared Error)`**입니다.

<br>

### 비용함수(Cost Function)의 추가적인 설명과 예시

- 비용 함수의 가장 이상적인 그래프, 즉 $$y$$ 값을 모두 통과(예측)하는 함수 그래프는 다음과 같습니다.
  아래 예시를 보시면 $$θ_1$$ 값에 1을 주었고, 그 결과 $$J(θ_0,θ_1)$$ 값은 0입니다. ($$y$$와 $$h(x)$$의 차가 없으므로 0입니다) 

  ![image-20210112211123353](C:\Users\steve\AppData\Roaming\Typora\typora-user-images\image-20210112211123353.png) 

<br>

- 두 번째 예시입니다. 여기서는 $$θ_1$$에 0.5를 주었으며, 비용함수 수식에 따라 계산해보면 $$J(θ_0,θ_1)$$ 값은 약 0.58 임을 알 수 있습니다.
  ![image](https://user-images.githubusercontent.com/61633137/104313142-f3a32c80-551a-11eb-8caa-0859fd4d167d.png) 

<br>

- 위 그래프 예시들을 보았을 때 우리가 찾아야 할 값은 $$θ_1= 1$$ 처럼 비용함수를 최소화하는 $$θ$$ 값을 찾아야 합니다.
  이 값을 찾는 것을 **`Minimization`**이라고 하며 $$θ_1= 1$$ 처럼 비용함수를 최소화하는 값을 **`극값(Global Minimum)`**이라고 합니다.

<br>

## 좀 더 복잡한 예시

- 이번에 나올 예시는 3차원 그래프와 등고선 그래프입니다. 

- 두 가지의 변수를 가진 함수는 각 선의 모든 점에서 값을 가집니다. 여기서 두 가지 변수를 가진다는 것은 $$\theta$$값이 두 개라는 의미이며 비용함수를 포함한 그래프를 그릴 때는 **`3차원 그래프`** 로 바뀌어야 합니다.

  <img src="https://user-images.githubusercontent.com/61633137/104315426-46321800-551e-11eb-9bd4-38eb69cfe8b2.png" alt="image" style="zoom: 75%;" /> $$θ_0$$는 50, $$\theta_1$$은 0.06일 때 검은색 직선이 그려진다.

  <img src="https://user-images.githubusercontent.com/61633137/104315508-6b268b00-551e-11eb-90e8-7a6bfbc8f76b.png" alt="image" style="zoom:61%;" /> 3차원 그래프로 표현할 시 세로 높이가 비용함수값인 $$J(\theta_0,\theta_1)$$이 된다.

<br>

- 등고선 그래프로 나타내면 다음과 같습니다.
  ![image](https://user-images.githubusercontent.com/61633137/104316743-43d0bd80-5520-11eb-84cf-fc531a9ae9e9.png)

  위 그래프에서 가설함수 $$h(x)$$는 파란선입니다. 이 가설함수를 등고선 그래프로 표현하면 오른쪽 그래프의 형태가 됩니다.
  여기서 현재의 가설함수(파란색 선)을 만든 $$\theta_0$$과 $$\theta_1$$은 빨간색 X 표시로 나타납니다.
  <br>

  이 등고선 그래프에 대해 추가 설명을 하자면 아래 그래프에 표시되어 있는 핑크색 X 표시들은 각각 다른 $$\theta$$ 값이지만 비용함수는 같다는 것을 의미합니다.
  ![image](https://user-images.githubusercontent.com/61633137/104317090-c8bbd700-5520-11eb-8dd2-f9d86a58ac81.png)

<br>

