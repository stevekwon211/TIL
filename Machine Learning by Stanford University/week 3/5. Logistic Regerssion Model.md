## 로지스틱 회귀의 비용함수

**`로지스틱 회귀`**에서 파라미터 $$\theta$$를 최적화하기 위한 비용함수에 대해 알아보겠습니다.

> 비용함수가 무엇인지 모르겠다면 ➡ [머신러닝? 선형 회귀는 뭐고 비용함수는 무엇인가요?](https://velog.io/@kwonhl0211/머신러닝-선형-회귀는-뭐고-비용함수는-무엇인가요)

> 로지스틱 회귀가 무엇인지 모르겠다면 ➡ [분류 문제와 결정 경계](https://velog.io/@kwonhl0211/분류-문제와-결정-경계)

설명에 앞서 수식들에 대해 정리를 해보겠습니다.

먼저 Training Set과 m의 예시입니다.
$$\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}$$
![image](https://user-images.githubusercontent.com/61633137/105946863-61b72a00-60ab-11eb-88d7-3e68400b9587.png)

로지스틱 함수입니다.
$$h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}$$

기존의 비용함수 수식입니다.
$$J(\theta)=\frac{1}{m}\sum^m_{i=1}\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2$$

이 비용함수는 **`선형 회귀`**에서 쓰이는데 **`로지스틱 회귀`**에선 조금 변형된 비용함수를 사용합니다. 

왜냐하면 로지스틱 함수는 결과값이 물결 모양을 그릴 것이며 많은 **`Local Optima`**을 만들 것이기 때문입니다. 

다시 말해 기존 비용함수는 로지스틱 회귀에서 **`볼록 함수(Convex Function)`**를 그리지 않고 다음과 같은 그래프를 그릴 것입니다.
![image](https://user-images.githubusercontent.com/61633137/105948907-2ae31300-60af-11eb-9c20-d0d380105eb7.png)


변형된 비용함수는 다음과 같습니다.
$$J(\theta)=\frac{1}{m}\sum^m_{i=1}Cost(h_\theta(x^{(i)}), y^{(i)})$$
만약 $$y=1$$이면, $$Cost(h_\theta(x), y)= -\log(h_\theta(x))$$
만약 $$y=0$$이면, $$Cost(h_\theta(x), y)= -\log(1-h_\theta(x))$$
![image](https://user-images.githubusercontent.com/61633137/105949224-b2308680-60af-11eb-83c2-3e36288fc21a.png)![image](https://user-images.githubusercontent.com/61633137/105948527-6df0b680-60ae-11eb-92d0-c92c87d76155.png)
그래프에 대해 추가 설명을 덧붙이자면
$$y=1$$일 때 $$h_\theta(x)$$가 0으로 예측되었다면 비용함수값은 무한대에 가까워지게 되는 것이고, 반대로 1로 예측되었다면 비용함수값은 0에 가까워지게 되는 것입니다.
$$y=0$$인 경우는 반대로 생각해보면 됩니다. 

<br>

***

## 비용함수 단순화와 로지스틱 회귀의 경사하강법

### Simplified Cost Function

위에서 살펴본 로지스틱 회귀에서의 비용함수는 어떻게 하나의 식으로 표현될 수 있는걸까요?

아래 수식이 $$y=1$$일 때와 $$0$$일 때의 수식을 합쳐서 표현한 로지스틱 비용함수입니다.
$$Cost(h_\theta(x), y)=-y\log(h_\theta(x))-(1-y)\log(1-h_\theta(x))$$

이에 대해 설명을 해보겠습니다.

$$y=1$$일 때는 $$-(1-y)\log(h_\theta(x))$$가 $$0$$이 되면서 남겨진 수식은 $$-1*\log(h_\theta(x))$$, 즉 $$-\log(h_\theta(x))$$가 되어 기존 $$y=1$$일 때의 수식이 됩니다.

$$y=0$$일 때는 $$-y\log(h_\theta(x))$$가 $$0$$이 되면서 남겨진 수식은 $$-(1-0)\log(1-h_\theta(x))$$, 즉 $$-\log(1-h_\theta(x))$$가 되어 기존 $$y=0$$일 떄의 수식이 됩니다.

<br>

***

### Gradient Descent

이전에 선형 회귀에서 배웠던 것처럼 로지스틱 회귀에서도 비용함수 $$J(\theta)$$를 최소로 하는 $$\theta$$값을 찾기 위해 경사하강법 알고리즘을 사용하게됩니다. 선형 회귀에서의 경사하강법 수식은 아래와 같습니다.

$$\begin{aligned} & Repeat \; \lbrace \\ & \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta) \\ & (simultaneously\ update\ all\ \theta_J )\\ & \rbrace \end{aligned}$$	

여기서 **`Learning Rate`**인 $$\alpha$$값 뒷부분을 로지스틱 함수로 바꾸면 됩니다.

$$\begin{aligned} & Repeat \; \lbrace \\ & \; \theta_j := \theta_j - \alpha \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\ & \rbrace \end{aligned}$$

<br>

***

## Advanced Optimization

경사하강법에서 비용함수를 최적의 $$\theta$$값을 구하려면 $$J(\theta)$$와 $$\frac{\partial}{\partial\theta_j}J(\theta)$$를 평가해야 합니다.

이 때 사용되는 방법은 경사하강법뿐만이 아닌 다른 방법들도 존재합니다.
- **`Conjugate Gradient`**
- **`BFGS`**
- **`L-BFGS`**

장점은 $$\alpha$$값을 직접 지정할 필요없으며 경사하강법보다 좋은 성능을 내는 경우도 있다는 것입니다.
단점으론 경사하강법보다 더 복잡하다는 것입니다.

강의에서 Andrew Ng 교수님은 나머지 알고리즘을 사용할 시 직접 구현하는 것보단 이미 존재하는 머신러닝 라이브러리를 활용하는 것을 추천하셨습니다.

<br>

***

> Coursera에 있는 Andrew Ng 교수님의 Machine Learning 강의를 듣고 정리하는 시리즈입니다!

> 저는 수학 베이스가 부족하여 글의 내용이 틀릴 수도 있습니다. 잘못된 내용이 있을 시 댓글로 알려주시면 감사하겠습니다!!
> (수학 공부는 열심히 하고 있어요! 😁)