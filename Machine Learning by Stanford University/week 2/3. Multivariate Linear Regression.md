# Multivariate Linear Regression, 하나 이상의 변수를 다루는 선형 회귀

- Linear regression Multiple variables, 즉 변수가 다양한 선형 회귀를 뜻하며 **`Multivariate Linear Regression(다변량 선형 회귀)`**라고 합니다.

- 아래 사진 예시를 보시면 $$x_1, x_2, x_3, x_4$$처럼 변수가 여러 개인 것을 볼 수 있습니다.
  이렇게 여러 features들이 있는 선형 회귀를 다변량 선형 회귀라고 하는 것입니다.

  ![image](https://user-images.githubusercontent.com/61633137/104907878-132dcf80-59c9-11eb-9712-98e3a2187a1c.png) 

<br>

***

설명에 앞서 다음은 각 변수의 표기법입니다.
$$
x_j^{(i)}= i번째 \ 훈련 \ 예의 \ j번째 \ 값
$$

$$
x^{(i)} = i번째 \ 트레이닝 \ 예의 \ 입력값
$$

$$
m = 트레이닝 \ 횟수
$$

$$
n = 특징의 \ 개수
$$

이전에 배웠던 변수가 하나인 **`단변량 선형 회귀 함수`**의 함수식은 다음과 같습니다.
$$
h_\theta(x)= \theta_0+\theta_1x
$$
변수가 여러 개인 **`다변량 선형 회귀 함수`**의 함수식은 다음과 같습니다.
$$
h_\theta(x) = \theta_0+\theta_1x_1 +\theta_2x_2 +\theta_3x_3+...+\theta_nx_n
$$
다변량 선형 회귀 함수는 다음과 같이 두 행렬의 곱으로 나타낼 수 있습니다.
또한 아래 식에서 수식을 간단히 하기 위해 $$x_0 = 1$$로 정의할 수 있습니다.
$$
h_\theta(x) = [\theta_0 \ \theta_1 \ ... \theta_n]\left[\begin{array}{} 
x_0\\
x_1\\
.\\
.\\
.\\
x_n\\
\end{array}\right] = \theta^Tx
$$

이해를 위한 예시로 집의 가격을 들어보자면 다음과 같습니다.
$$
\theta_0은 \ 집의 \ 기본 \ 가격 \\
\theta_1은 \ m^2당 \ 가격 \\
\theta_2은 \ 층 \ 개수 \ 당 \ 가격 \\
\ \\
x_1은 \ 집의\ 넓이(m^2) \\
x_2는 \ 층의 \ 개수 \
$$

<br>

***

## 여러 변수에 경사하강법을 어떻게 적용하나요?

설명에 앞서 지금까지 배운 수식들을 한 번 더 정리해보겠습니다.

- Hypothesis: $$h_\theta(x) = \theta^Tx = \theta_0x_0 + \theta_1x_1+ \theta_2x_2 + ... + \theta_nx_n
- Parameters: $$\theta_0, \theta_1, ... , \theta_n = \theta$$
- Cost function: $$J(\theta_0,\theta_1,...,\theta_n) = \frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})2 = J(\theta)$$
- Gradient descent: $$Repeat { \\ \theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta) \\ } (simultaneously \ update \ for \ every \ j=0,...,n)$$

<br>

기존의 경사하강법 알고리즘은 다음과 같습니다. 이 알고리즘은 입력변수가 1개일 때의 식입니다.
$$
repeat \ until \ convergence\{ \\
    \theta_0 := \theta_0 - \alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)}) \\ 
    \theta_1 := \theta_1 - \alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x^{(i)} \\
    \}
$$
하지만 이 알고리즘은 입력변수가 1개 이상이 되면 사용될 수가 없습니다. 
그래서 입력변수가 1개 이상일 때의 알고리즘은 다음과 같습니다.
$$
\theta_0 := \theta_0 - \alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)}-y^{(i)})x_0^{(i)} \\ 
    \theta_1 := \theta_1 - \alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x_1^{(i)} \\
    \theta_2 := \theta_2 - \alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x_2^{(i)} \\
$$
 그리고 이 알고리즘을 일반화하면 다음 식으로 정의됩니다.
$$
repeat \{ \\
\theta_j := \theta_j - \alpha\frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\
\} \ simultaneously \ update \ \theta_j \ for \ j=0,...,n
$$
<br>

***

## 더 효과적인 경사하강법 - 1. Feature Scaling

- **`Feature Scaling`**은 경사하강법의 Tip 중 하나입니다. 
  원래 Features의 범위와 대략 비슷한 범위이면서 수는 더 작게 조정해 주는 것을 뜻하는데요.
  조정을 하는 이유는 $$\theta$$ 값이 작으면 작을수록 빠르게 경사하강을 할 수 있고 크면 클수록 느리게 움직이기 때문입니다.

<br>

***

### Feature Scaling 예시

하나의 예를 들어보겠습니다.

집의 크기 : $$x_1 =$$ size(0~2000 $$feet^2$$)
방의 개수 : $$x_2 =$$ number of bedrooms (1~5)

라고 할 때 $$x_i$$를 $$x_i$$의 최댓값으로 나눌 시, 
$$x_1$$의 범위가 0~2000 이어서 매우 뾰족한 타원 그래프(비용함수)가 만들어집니다.
![image](https://user-images.githubusercontent.com/61633137/105247269-decc3600-5bb7-11eb-87da-07ce1bea8f21.png) 

여기서 
$$x_1 = \frac{size(feet^2)}{2000}$$
$$x_2 = \frac{number \ of \ bedrooms}{5}$$
처럼 Feature Scaling을 해주면 $$x_1$$과 $$x_2$$ 모두 0~1의 범위로 비율이 비슷하게 만들어집니다.
![image](https://user-images.githubusercontent.com/61633137/105247642-729e0200-5bb8-11eb-8914-4b6a61ba099d.png)
위 그래프보다 훨씬 정확하고 빠르게 최솟값을 찾아갈 수 있습니다.

위 예시와 같이 일반적으로 Feature Scaling을 할 때 $$-1 \leq x_{(i)} \leq 1$$ 또는 $$-0.5 \leq x_{(i)} \leq 0.5$$ 정도의 범위에 들어오게 만드는데 $$0 \leq x_{(i)} \leq 3$$ 이나 $$-2 \leq x_{(i)} \leq 0.5$$ 정도의 비율이면 문제없다고 합니다.

<br>

***

### Mean Normalization

- **`Mean Normalization`**은 모든 features들이 $$-1 \leq x_i \leq 1$$ 범위에 들어가도록 하는 것입니다.
  $$
  \mu_i = 모든 \ 값의 \ 평균 \\
  S_i = (최댓값 - 최솟값) \ 또는 \ 표준편차 \\
  \ \\
  x_i := \frac{x_i-\mu_i}{S_i}
  $$
  위 식을 통해 간단하게 Scaling을 할 수도 있다.

<br>

***

## 더 효과적인 경사하강법 - 2. Learning Rate

<img src="https://user-images.githubusercontent.com/61633137/105551582-2e406c80-5d46-11eb-9972-ca2e99c35ddd.png" alt="image" style="zoom:67%;" /> 

**`Debugging Gradient Descent`** : 위 그래프처럼 X축에 경사하강법이 반복되는 횟수, Y축에는 $$J(\theta)$$를 나타내는 그래프를 그립니다. 

만약 아래 그래프처럼 $$J(\theta)$$ 값이 오른다면 $$\alpha$$값을 낮춰야 합니다.
![image](https://user-images.githubusercontent.com/61633137/105551297-e0c3ff80-5d45-11eb-87bf-23a00006cb18.png) <img src="https://user-images.githubusercontent.com/61633137/105551341-f20d0c00-5d45-11eb-879e-6001b99cad1c.png" alt="image" style="zoom: 75%;" /> 
하지만 $$\alpha$$값이 너무 작다면 경사하강법이 최솟값에 수렴하는 속도가 느려질 수 있습니다.
$$\alpha$$는 0.001에서 시작해서 3배씩 그 수를 늘려가면서 0.003, 0.009(0.01), 0.027, 0.081, 0.1, 0.243, 0.729, 1 ... 같은 값으로 시도해보는 것이 좋다고 합니다.

**`Automatic Convergence Test`** : 경사하강법이 최솟값에 수렴하는지 테스트하려면 경사하강법을 반복할 때마다 $$J\theta$$값이 $$\Epsilon(10^{-3})$$값보다 작은지 확인하는 것을 말합니다. 하지만 이런 threshold(임곗값)은 정하기가 어려워 Andrew Ng 교수는 위 예시처럼 그래프를 그려보는 것을 추천했습니다.

결론은 다음과 같습니다.

- $$\alpha$$값이 너무 작으면 천천히 수렴합니다.
- $$\alpha$$값이 너무 크면 $$J\theta$$값이 반복할 때마다 줄어들지 않을 수 있고 이는 수렴하지 않는다는 것을 뜻합니다.
  천천히 수렴할 가능성도 있습니다.

<br>

***

## Features and Polynomial Regression, 다항식 선형 회귀란?

- **`Feature`** 값들과 **`가설 함수`**를 개선할 수 있는 몇 가지 방법 중 하나입니다.
- 여러 Feature 값들을 하나로 결합하는 방식인데, 예를 들면 $$x_1$$과 $$x_2$$를 결합하여 $$x_3$$ Feature를 만드는 것입니다.

가설 함수는 꼭 선형이 아니어도 됩니다. 왜냐하면 가설 함수를 **`Quadratic(이차함수)`**, **`Cubic(삼차함수)`**, **`Square Root(루트함수)`** 등의 형태로 변환하면 되기 때문입니다. 

***

강의에서 **`Housing Prices Prediction`**을 예시로 들었습니다.

![image](https://user-images.githubusercontent.com/61633137/105554355-811c2300-5d4a-11eb-8e96-44e41b8b1896.png) 

$$h_\theta(x) = \theta_0 + \theta_1 * frontage + \theta_2 * depth$$

위와 같은 가설 함수에서 Feature는 frontage와 depth입니다.
여기서 frontage와 depth는 각각 가로와 세로를 나타내며 이 둘을 곱하여 Size(집의 넓이) $$x$$라는 Feature를 만들 수 있습니다.

이를 새로운 가설 함수 $$h(x) = \theta_0 + \theta_1x$$가 만들어졌습니다.

![image](https://user-images.githubusercontent.com/61633137/105554908-a8bfbb00-5d4b-11eb-91cf-bc239569d3a9.png) 

위 예시에서는 새로운 가설함수를 이차함수가 아닌 삼차함수 형태를 적용하면 집의 크기에 따라 집의 가격이 증가한다고 예측하는 것을 볼 수 있습니다. 집의 크기가 넓어질수록 가격이 떨어지는 것은 논리적으로 맞지 않기 때문에 삼차함수가 알맞은 가설함수라고 생각됩니다.

삼차함수 형태가 아니더라도 루트함수의 형태로도 나타낼 수 있습니다.
$$h_\theta(x) = \theta_0 + \theta_1(size) + \theta_2\sqrt{(size)}$$
![image](https://user-images.githubusercontent.com/61633137/105555548-f4269900-5d4c-11eb-8e8e-ec52a00ab220.png)

<br>

***
> Coursera에 있는 Andrew Ng 교수님의 Machine Learning 강의를 듣고 정리하는 시리즈입니다!

> 저는 수학 베이스가 부족하여 글의 내용이 틀릴 수도 있습니다. 잘못된 내용이 있을 시 댓글로 알려주시면 감사하겠습니다!!
> (수학 공부는 열심히 하고 있어요! 😁)