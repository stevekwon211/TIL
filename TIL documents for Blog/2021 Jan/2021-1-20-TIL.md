> _TIL은 오늘 공부한 것을 간략하게 요약한 내용만 적습니다.
> 자세한 내용은 시리즈별로 포스팅합니다._

## 💻Data Science

**`[Coursera - Machine Learning by Andrew Ng 2주차]: 복습 및 공부중`**

**`[Kaggle Course - Intermediate Machine Learning]: XGBoost 완료`**

XGBoost : Ensemble method(앙상블 기법: 여러 분류모델의 예측을 결합하는 기법) 중 하나입니다.
Gradient Boosting : Ensemble에 모델을 반복적으로 추가하는 것을 말합니다.

Gradient Boosting의 단계
![image](https://user-images.githubusercontent.com/61633137/105176649-d7ca0700-5b68-11eb-85c1-fd747a5d11fb.png)

1. 먼저 현재 앙상블을 사용하여 데이터셋에 대한 예측을 생성합니다. 또한 앙상블에 있는 모든 모델의 예측을 계속해서 추가합니다.
2. 이러한 예측을 이용하여 손실 함수(예: 평균 제곱 오차)를 계산하는 데 사용합니다.
3. 이 손실 함수를 사용하여 앙상블에 추가될 새 모델을 훈련시킵니다. 또한 이 새로운 모델을 앙상블에 추가할 때 손실이 줄어들도록 모델 파라미터를 결정합니다. 
4. 새로운 모델을 앙상블에 추가해서 과정을 반복합니다.



<br>

***

## 🌠Kaggle

**`Tabular Playground Series - Jan 2021 참여중 `**
**`현재 점수 : 0.70714`**
**`현재 순위  746/1188`**

맨 처음 시작했을 때 점수가 대략 **`0.78XXX`** 이었는데 현재 **`0.70714`** 까지 끌어올렸습니다..!! 
참고로 Tabular Playground Series - Jan 2021은 RMSE(평균 제곱근 편차)를 평가방식으로 도입해서 숫자가 낮을 수록 좋은 점수가 됩니다.
$$RMSE=\sqrt{\frac{1}{n}\sum^n_{i=1}(y_i-\hat y_i)^2}$$

<br>

***

## ⛓Algorithm

**`[solved.ac - Bronze V] : 3046, 5337, 5338, 5339 풀기`**

![image](https://user-images.githubusercontent.com/61633137/105179519-c71b9000-5b6c-11eb-8595-4ad6af82729f.png) 

<br>

***

## 📐Math

**`Loading...`**

<br>

***

## 👾J2KB

**`SSWEB 주요 기능 열심히 개발중...`**

<br>

***

## 👨‍💻일상

동생이랑 핸드폰을 바꿨습니다. 
저는 갤럭시 노트9이고 동생은 아이폰7인데 제가 노트9이 질리고 좀 작은 핸드폰을 쓰고 싶어서 바꿨습니다 ㅋㅋㅋㅋ
일단 지금까진 만족중입니다 ㅎㅎ